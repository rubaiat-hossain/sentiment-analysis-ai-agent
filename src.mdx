# Monitoring AI Agents with Helicone

As AI agents become more common in apps and services, it's essential to understand how they behave. In this guide, we'll build a simple sentiment analysis agent powered by Groq for fast LLM responses and wrap it with the Model Context Protocol (MCP) so you can easily use it with tools like Cursor. Then, we'll integrate Helicone to monitor and debug the agent in real-time, giving you a clear view of how it performs and responds.

## Setting Up the Sentiment Analysis Agent

Our simple AI agent takes any sentence as its input and returns the sentiment, classified as positive, neutral, or negative. The agent also gives users a confidence score and a brief explanation of its reasoning. It uses Groq's Llama 4 model to generate the analysis, and we wrap the model interaction inside a Model Context Protocol (MCP) tool so it's compatible with platforms like Cursor.

We'll also route all LLM traffic through Helicone, which acts as a proxy and observability layer, letting us track requests, sessions, latencies, and more.

### Prerequisites

To follow along, you’ll need the following tools and API keys configured.

* Node.js v18+
* Groq API key – https://console.groq.com
* Helicone API key – https://www.helicone.ai

### Step 1. Initialize the project

Begin by creating a new directory for your project and initializing it using the following commands.

```bash
mkdir sentiment-agent
cd sentiment-agent
npm init -y
```

### Step 2. Install the dependencies

You need to install the necessary packages for our AI agent using these commands.

```bash
npm install express dotenv zod groq-sdk @modelcontextprotocol/sdk
npm install -D typescript tsx @types/node @types/express
```

### Step 3. Configure TypeScript

Initialize TypeScript configuration using -

```bash
npx tsc --init
```

### Step 4. Create your agent

Create a `main.ts` file and add the following code to it.

```ts
import { config } from "dotenv";
config();

import express from "express";
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp";
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse";
import { z } from "zod";
import Groq from "groq-sdk";
import { randomUUID } from "crypto";

// Initialize Groq with Helicone proxy headers
const sessionId = randomUUID();
const sessionName = "Sentiment Session";

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY || "",
  baseURL: "https://groq.helicone.ai",
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY || ""}`,
    "Helicone-Session-Id": sessionId,
    "Helicone-Session-Name": sessionName,
    "Helicone-Session-Path": `/analysis/${Date.now()}`,
  },
});


// Initialize MCP Server
const server = new McpServer({
  name: "Sentiment Analysis Agent",
  version: "1.0.0",
});

// Define the sentiment analysis tool
server.tool(
  "analyzeSentiment",
  { text: z.string().min(1).max(1000) },
  async ({ text }) => {
    try {
      const sanitizedText = text.trim();
      if (!sanitizedText) {
        return {
          content: [
            { type: "text", text: "Error: Empty text provided" },
          ],
        };
      }

      // Send prompt to Groq via Helicone
      const completion = await groq.chat.completions.create(
        {
          messages: [
            {
              role: "user",
              content: `Analyze the sentiment of the following text and respond in JSON format with fields for "sentiment" (positive/neutral/negative), "confidence" (0-1), and "explanation". Text: "${sanitizedText}"`,
            },
          ],
          model: "meta-llama/llama-4-scout-17b-16e-instruct",
          temperature: 0.4,
          max_tokens: 1000,
        },
        {
          headers: {
            "Helicone-Session-Id": sessionId,
            "Helicone-Session-Name": sessionName,
            "Helicone-Session-Path": `/analysis/${Date.now()}`, // unique per request
          },
        }
      );

      const responseText = completion.choices?.[0]?.message?.content || "";

      let parsedResponse;
      try {
        const match = responseText.match(/\{[\s\S]*\}/);
        parsedResponse = match ? JSON.parse(match[0]) : null;
      } catch (e) {
        parsedResponse = null;
      }

      return {
        content: [
          {
            type: "text",
            text: parsedResponse
              ? `Sentiment: ${parsedResponse.sentiment}\nConfidence: ${parsedResponse.confidence}\n\n${parsedResponse.explanation}`
              : responseText,
          },
        ],
      };
    } catch (err: any) {
      console.error("[MCP] Sentiment error:", err);
      return {
        content: [
          {
            type: "text",
            text: `Error: ${err.message || String(err)}`,
          },
        ],
      };
    }
  }
);

const app = express();
let transport: SSEServerTransport | null = null;

// Set up SSE endpoint
app.get("/sse", (req, res) => {
  transport = new SSEServerTransport("/messages", res);
  server.connect(transport);
});

app.post("/messages", (req, res) => {
  if (transport && typeof transport.handlePostMessage === "function") {
    transport.handlePostMessage(req, res);
  } else {
    res.status(500).send("Transport not initialized or method unavailable");
  }
});

app.listen(3000, () => {
  console.log("Sentiment Agent MCP running at http://localhost:3000/sse");
});
```

### Step 5. Configure environment variables

Create a `.env` file and add your API keys and the Helicone base URL needed for capturing sessions.

```bash
GROQ_API_KEY=your_groq_api_key
HELICONE_API_KEY=your_helicone_api_key
HELICONE_BASE_URL=https://oai.helicone.ai/v1
```

### Step 6. Run the agent

With your environment variables configured, you can now start the agent.

```bash
npx tsx main.ts
```

If you've set up the environment correctly, you'll see -

`Sentiment Agent MCP running at http://localhost:3000/sse`

This command starts the agent with an MCP-compatible server, ready to receive messages using the MCP protocol via Server-Sent Events (SSE).

### Step 7. Add agent to Cursor

Your AI agent is accessible over MCP in supported environments like Cursor. You can easily add the agent to your IDE, issue prompts, and receive outputs directly.

In Cursor, go to **Settings > MCP > Add new global MCP server** and paste the configuration.

```json
{
  "mcpServers": {
    "AI Agent": {
      "url": "http://localhost:3000/sse",
      "readTimeoutMs": 300000,
      "connectTimeoutMs": 30000,
      "reconnectionTimeoutMs": 5000,
      "method": "GET"
    }
  }
}
```

This configuration sets up your AI agent server as a local process that communicates with Cursor via standard input/output streams.

### Step 8. Run the agent

Once you've successfully integrated the AI agent as an MCP tool in Cursor, you're ready to prompt your agent and analyze text sentiment.

"Analyze this: I love the new design; it's clean and intuitive."

Your agent will call the Groq LLM behind the scenes, analyze the sentiment, and respond with a structured message like:

```text
The sentiment analysis of "I love the new design; it's clean and intuitive" shows:

- Sentiment: Positive
- Confidence: 0.9 (very high)

The positive sentiment is driven by the strong affective word "love" combined with the complimentary descriptors "clean and intuitive" - indicating high satisfaction with the design.
```
![](https://i.imgur.com/e5YIqHC.png)

## Monitor Your AI Agent with Helicone

Now that your sentiment analysis agent is running, let's add observability so you precisely understand how it works behind the scenes.

We're using Helicone as our observability layer. It acts as a proxy between your AI agent and Groq's API, capturing every request and response. This gives you a complete picture of how your AI agent is performing without changing your app's core logic.

By sending requests to Groq through the Helicone proxy, we automatically have access to -

* Full request/response logging
* Latency metrics
* Usage tracking
* Sessions - a group of related user interactions for debugging and analysis

You've already set up the Helicone proxy when initializing Groq API connections in your `main.ts` code.

```ts
const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY || "",
  baseURL: process.env.HELICONE_BASE_URL,
  defaultHeaders: {
    "Helicone-Auth": `Bearer ${process.env.HELICONE_API_KEY || ""}`,
  },
});
```

This setup routes all traffic through Helicone, enabling robust monitoring with zero additional code.

Once you send a few test prompts via Cursor, visit your Helicone dashboard, and you'll see your AI usage details like this -

![](https://i.imgur.com/0RIFOGz.png)

Navigate to the Sessions menu from the left pane, and there you can click into a session to view -

* View raw request and response data
* Inspect latency and token usage
* Compare different inputs side-by-side

![](https://i.imgur.com/s9CUOE2.png)

This feature is handy when debugging an interaction or analyzing usage patterns over time.

## Conclusion

Observability is key to building reliable AI agents. Your AI app can't succeed without visibility into how your agent behaves, what it sees, how it responds, and where it might fail.

In this tutorial, we built a simple sentiment analysis agent using Groq for the AI model and wrapped it with MCP for easy integration. By adding Helicone, we gained deep observability into every request, with sessions showing exactly how the agent responds in context.
